<!-- LB - update - 12/01/2022 -->
#  Model selection {#modelselection}

## Introduction {#INTmodsel}

If there is only one, or a small number of possible explanatory variables (also known as covariates, predictors or independent variables) choosing a linear regression model can be straightforward. What happens when there are many explanatory variables? Some covariates may be more important/useful than others in explaining the response variable. Since conclusions depend, to some extent, on the covariates in the linear model, how do we decide which covariates to include? This chapter considers how to choose between competing models using different model selection procedures: $p$-values, fit scores and automated methods. The concepts described in this chapter use examples introduced in Chapter \@ref(introlm).   

### Criteria for model selection

We want to have an _appropriate_ set of covariates in our model: 

+ if we include too few variables we throw away valuable information, and
+ if we include non-essential variables the standard errors and $p$-values tend to be too large.
+ if the models are too simple (_under-fitted_), or too complex (_over-fitted_), the models will have poor predictive abilities.

We want to include variables which: 

+ have a genuine relationship with the response and
+ offer a sufficient amount of new information about the response (given the variables already included) 		

We want to exclude variables that:

+ offer essentially the same information about the response; e.g., we want to avoid __collinearity__.

## Collinearity

Collinearity is a *linear* association between two variables in a linear regression model. 'Multicollinearity' refers to linear associations between two or more variables in a multiple regression model.

When 'collinear' variables are fitted together in a model, 

+ the resulting model is unstable (because we are trying to estimate two parameters when one will do) and, 
+ we obtain inflated standard errors for these estimates.

We have methods to detect this however. 

### Variance inflation factors

Collinearity can be detected using 'variance inflation factors' (VIFs). These are based on fitting linear models between each covariate (in turn) and the remaining covariates and assessing the predictive power of each: 

\begin{equation}
VIF_p=\frac{1}{1-R_p^2}
\end{equation}

where $R^2_p$ is the squared correlation between the $p$-th observed covariate value and those predicted by a linear model containing the other covariates. If any of the $R^2_p$ values are  high, then the VIF will also be high.

There are no firm rules about how large VIFs need to be before remedial action (e.g. removing a covariate) is required; some say VIFs $> 5$, some say VIFs $>10$.

VIFs require adjustment if we estimate multiple parameters (i.e. regression coefficients) for a particular covariate, for example, the number of regression coefficients estimated for a factor is the 'number of levels - 1'. However, VIFs are easily calculated using software. 


#### Doing this in R

We consider a regression of total height on left leg length (LLL) and right leg length (RLL) (seen in Chapter  \@ref(introlm)). The covariates LLL and RLL are strongly correlated but the variable length of hair (LOH) is not correlated with left leg length (see below) or indeed total height. 

```{r, echo=F, eval=T}
set.seed (101)
LLL <- rnorm (100,110,4)
RLL <- rnorm(100, LLL,0.05)
LOH <- rpois (100,20)

TotalHeight <- (LLL+RLL)/2*1.8/1.1 +rnorm (100,0,4)
```

```{r, echo=F, eval=T, fig.cap="Relationship between length of hair and left leg length."}
plot (LOH, LLL, ylab="Left leg length (cm)", xlab="Length of hair (cm)")
```

A function to obtain VIFs is available in the `car` package. To calculate the VIFs, we need to first fit a linear model with all the potential explanatory variables included.  

```{r, echo=T, eval=T}
# Fit multiple regression model
modelAll <- lm(TotalHeight ~ LLL + RLL + LOH)
# Load package
require(car)
# Calculate VIFs
vif(modelAll)
```

The large values of the VIFs for LLL and RLL indicate that they are (not surprisingly) highly correlated. This suggests removal of one of the leg length variables. The variable RLL is removed from the model and the process is repeated. The resulting VIFs indicate that the remaining variables are not collinear. 

```{r, echo=T, eval=T}
# Fit multiple regression model
model2vars <- lm(TotalHeight ~ LLL + LOH)
# Calculate VIFs
vif(model2vars)
```

NOTE: Variables should be removed one at a time and everything retested. 

#### Dealing with collinearity

Collinearity can be addressed by removing one of the collinear variables, but alternative methods exist if it desirable to retain the full set of covariates. 

The removal of one, or more, collinear covariates may occur automatically if $p$-values (see below) are used to drop terms from a model. This occurs since collinear terms are often unstable and thus highly uncertain, which means the associated $p$-values are often large. A 5 minute clip about this issue can be found [here](<http://www.youtube.com/watch?v=O4jDva9B3fw>).
A large $p$-value could mean either the variable has no effect, or is correlated with another predictor.

Alternatively, the analyst may use their judgement in which collinear covariate(s) are retained and which are omitted (but readers might be skeptical of such a subjective approach). This can be done, for instance, by comparing the relative predictive power of the model with and without each covariate and choosing the covariate which predicts the response `best'.

**Q15.1** Does a low VIF indicate that a variable should be in the final model? 

## $p$-value based model selection: the $F$ test

Collinearity identifies correlated predictor variables, it does not necessarily generate a good model. The $p$-value associated with an estimated regression coefficient can be used to decide whether to include a particular variable in the final model. Essentially, we perform a hypothesis test where the null hypothesis is that the regression coefficient is equal to zero; a regression coefficient equal to zero would have the effect of eliminating that variable from the model. The $p$-value associated with a relevant test statistic is then interpreted in the usual way. 

+ For covariates with one associated coefficient, retention can be based on  the value of the associated $p$-value (i.e. large $p$-values suggest omission, small $p$-values (e.g. $<$ 0.05) suggest retention). 

+ For variables with multiple coefficients (e.g. factors) we are interested in assessing a group of coefficients simultaneously. In chapter 14, a model was fitted which included `month` as a factor variable; it had four levels and so there were three regression coefficients associated with it - these were denoted in the model as $\beta_5$, $\beta_6$, $\beta_7$. The test of interest is:

\begin{align*}
H_0:~& \beta_5=\beta_{6}=\beta_{7}=0\\
H_1:~& \textrm{ at least one of } \beta_5,\beta_{6},\beta_{7}  \neq
0
\end{align*}

We look at an example of this now. 

#### Comparing models with and without a factor

In fitting a linear model to the EIA data, `Month` was treated as a factor variable with four levels, therefore, models with and without `Month` differ by 3 parameters. We wish to compare a reduced model (without month) with the full model (with month) using a significance test. The idea is as follows:

+  If month is an important predictor then a model with month should predict the response values considerably  better than a model without month.
+ If a model with and without month are equivalent then models with and without month will predict the response data similarly.

We can use an $F$ test to formally test the hypothesis that the model without month (with $q$ parameters) is as good as the model with month (with $P$ parameters) and hence the smaller model is preferred (and month is not required).  
\small
```{r, eval=T, echo=F}
wfdata<- read.csv("data/NystedFarmsAnalysis.csv", header=T)
```

```{r, eval=T, echo=F}
# Fit model with month
modelEIAall <- lm(Density ~ XPos + YPos + DistCoast + Depth + Phase + as.factor(Month), data=wfdata)
# Analysis of variance
anova (modelEIAall)
```

Note that the explained sum of squares produced by the `anova` command is **sequential** so if we want to test for `Month` given all the other terms in the model i.e `Month` must come last in the list of variables (as it is in this example).  

__Differences in average density across months__

What can we conclude from these results? 

+ This test provides a large $F$ test statistic (21.2102) and small associated $p$-value ($p<0.0001$) for `Month`, which suggests a model with `Month` is significantly better than a model without `Month`.
+ This indicates genuine month to month differences in average density and thus month information should be retained in the model.

__Differences in average density across phases__

Phase of construction was included as another factor variable in the model; this had three levels (denoted by A, B and C). If we re-order the model so `Phase` is last, we can consider whether `Phase` should be in the model. 

```{r, eval=T, echo=F}
modelEIAall2 <- lm(Density ~ XPos + YPos + DistCoast + Depth + as.factor(Month) + Phase, data=wfdata)
anova(modelEIAall2)
```

+ For phase, the $F$ test results are quite the opposite to that for `Month`: $F=0.2409$ and $p=0.7859$. This indicates no significant difference between models with and without `Phase` and, therefore, no genuine differences in average density across phases. 

+ This suggests phase should be omitted from the model given the presence of the other variables.\footnote{A 7 minute clip about the $F$ test can be found online: \newline <http://www.youtube.com/watch?v=orGhAoQvSOM>}

In the above examples, we have written the model with the covariates in different orders to ascertain the appropriate $p$-values. The R function `Anova` (as opposed to `anova`) gives the $p$-values for all covariates assuming the term is the last in the model. Thus: 

```{r, eval=T, echo=F}
modelEIAall <- lm(Density ~ XPos + YPos + DistCoast + Depth + Phase + as.factor(Month) + Depth, data=wfdata)
Anova (modelEIAall)
```

The $p$-values are now equivalent to having the relevant term as the last term in the model, even though `Depth` is the actual last term now. Note the brackets with the words `Type II tests` - this indicates the so called 'Type II' sum of square's are being used (sums of squares that assume each term is the last in the model) as opposed to 'Type I'. Type I errors use the sequential sum of squares (which is affected by the order of the variables in the model specification) and are provided by the function `anova`. 

## Relative model fit

While $F$ tests (and associated $p$-values) can be used to compare __nested__ models (when one model is a special case of the other) they cannot be used to compare models which are not nested. In contrast, both nested and non-nested models can be compared using `information-based' fit criteria such as the AIC or BIC statistic.

### Akaike's Information Criterion (AIC)

Occam's Razor  is the very useful rule that when comparing models of equal explanatory power (i.e. models have the same $R^2$), that one should choose the simplest (i.e. fewest parameters). But what if the models have different levels of complexity along with different levels of explanatory power? Is a simple model of low explanatory power better than a complicated model with high explanatory power?

The AIC statistic is a fit measure which is penalized for the number of parameters estimated in a model;

+ a smaller AIC value signals a better model. 

The AIC is calculated using:

$$\textrm{AIC} =  \textrm{Fit to the data} +  \textrm{model complexity}$$

\begin{equation}\label{eq:aic}
AIC= -2 \textrm{ log-likelihood value} + 2P
\end{equation}
Where

 +  'Fit to the data' is measured using the so-called `log-likelihood' value\footnote{$\sum_{i=1}^{s}\sum_{t=1}^{n_i} \left( \frac{y_{it}\hat{y}_{it}-\hat{y}_{it}/2}{\sigma^2}-\frac{{y}_{it}}{2\sigma^2}-\frac{1}{2}\log (2\pi \sigma^2)\right)$ } calculated using the estimated parameters in the model,   
 +  'model complexity' is measured by $2P$ where $P$ is the number of covariates used to fit the model.

#### AICc

When the sample size is not a great deal larger than the number of parameters in the model, the small sample corrected AICc is a better measure than AIC:

\begin{equation}\label{eq:aicc}
AICc=AIC+\frac{2P(P+1)}{n-P-1}
\end{equation}

This value gets very close to the AIC score when sample size, $n$, is much larger than $P$.

#### BIC

The BIC score differs from the AIC score by employing a penalty that changes with the sample size ($n$):

\begin{equation}
BIC=-2 \textrm{log-likelihood value}  + \log(n)P
\end{equation}

As for the AIC and AICc, smaller values signal `better' models. BIC is more conservative than AIC and will produce models with fewer variables. 


## Other methods of model selection

Information criterion and $p$-values represent two approaches to model selection both based on likelihood. However there are other methods. Cross-validation is a method used both by statisticians and data scientists, where a model is fitted to a subset (the "training set") of the available data and then tested ("validated") against the remainder of the dataset, the "validation set". There a variety of forms of cross-validation.


## Automated variable selection

The number of possible combinations soon increases as the number of explanatory variables increases. There are various procedures described below which can be used to select the 'best' model. All of the procedures could be implemented manually (i.e. by fitting a model, obtaining a test statistic, fitting the next model, etc.) but this can be very time-consuming. Fortunately, there are R functions available which can be used to implement them.

### Stepwise selection

__Stepwise__ selection is a commonly used automated method which adds and drops covariates (from some start point) one at a time until no change occurs in the selected model. 

+ 'Importance' of variables can be measured in a number of ways; the AIC/AICc and BIC statistics are commonly used, alternatively $p$-values could be used. 

Selection proceeds either:

+ __forwards__ from a simple model by addition of covariates, or 
+ __backwards__ from a complex model by dropping covariates.
+ More elaborate algorithms are possible.

In forward selection, one variable is added at a time and the AIC etc. is calculated. Then another variable is tried INSTEAD, until all the candidate variables have been tried. The model with the lowest AIC (if lower than the starting model) is selected. A new 'round' of selection then begins with the remaining candidate models considered. Modelling proceeds until no further reduction in AIC is found.

In backward selection, starting from a model with all potential variables, one variable is removed and the AIC etc. is calculated. Then another variable is removed INSTEAD, until all the candidate variables have been tried. The model with the lowest AIC (if lower than the existing start model) is selected. A new 'round' then begins with the remaining candidate variables removed one at time then replaced as before. Modelling proceeds until no further reduction in AIC is found. 

These two methods will not necessarily select the same model because different combinations of variables are being included in the considered models. 

### All possible subsets selection

Rather than rely on an algorithm to determine the order in which variables are picked and dropped from a model (which can affect which covariates are retained), we can compare 'fit scores' for all possible models.

For example, for a 4 covariate model we can compare the fit scores for:

+  an intercept only model (1 model) 
+  all models containing  one covariate (4 models)
+  all models containing two covariates (6 models) 
+  all models containing three covariates (4 models)
+  the full model with 4 covariates (1 model)

However, this method becomes prohibitively time consuming when there are a lot of covariates.

#### Doing this in R

One way to fit all possible models is to use the `dredge` function inside the `MuMIn` library. The default score for ranking models is the AICc (but this can easily be changed) and the output includes all possible models which are ordered by AICc. 

As an example, we return to a model fitted to the EIA data with six potential explanatory variables. The `dredge` function has been used; the R code is shown (but not executed because it generates a lot of output). The regression coefficients for models with the four lowest AICc scores are shown below. :

```{r, eval=F, echo=TRUE}
require(MuMIn)
```

```{r, eval=F, echo=TRUE}
options(na.action = "na.fail")
dredge(modelEIAall)
```

\tiny 
```{r, eval=F, echo=TRUE}
Fixed term is "(Intercept)"
Global model call: lm(formula = Density ~ XPos + YPos + DistCoast + Depth + Phase + 
    as.factor(Month) + Depth, data = wfdata)
---
Model selection table 
   (Int) as.fct(Mnt)     Dpt     DsC Phs    XPs     YPs df    logLik     AICc delta weight
64  3286           + -0.4544 -0.3152   + 0.1175 -0.5549 11 -149489.8 299001.6  0.00  0.983
56  3288           + -0.4532 -0.3213     0.1185 -0.5553  9 -149495.9 299009.7  8.16  0.017
60  2702           + -0.5575           + 0.1009 -0.4567 10 -149500.1 299020.2 18.67  0.000
52  2692           + -0.5583             0.1016 -0.4552  8 -149506.6 299029.2 27.65  0.000
63  3260             -0.4524 -0.3117   + 0.1184 -0.5505  8 -149521.9 299059.7 58.16  0.000
55  3264             -0.4513 -0.3174     0.1193 -0.5513  6 -149525.7 299063.4 61.82  0.000
Models ranked by AICc(x)
```
\normalsize

We observe: 

+ the best model (by AICc) is at the top,
+ estimated regression coefficients are displayed for continuous variables and a `+`  for factor variables; neither a coefficient or a `+` indicates the variable is excluded. For example, line 3 shows `DistCoast` omitted and line 5 shows `Month` omitted. 
+ `delta` is the difference in AICc between the best (top) model and the listed model,
+ `weight` associated with fit score (described below).

One thing to note is that records with any missing values need to be excluded before using `dredge` because models with different numbers of observations cannot be compared with information criteria.. 

#### AIC weights

It is always important to be sensible about the covariates considered for selection, but this method also allows model comparison using weights based on your chosen fit score (e.g. AIC/AICc/BIC). 

These weights are based on the relative size of the difference between the fit of each candidate model and the best model using your chosen fit score. For example, using the AIC, weights are given by:

\begin{equation}\label{AICWeights}
w_i(AIC)=\frac{\exp\{-\frac{1}{2}\Delta_i(AIC)\}}{\sum_{k=1}^K\exp\{-\frac{1}{2}\Delta_k(AIC)\}}
\end{equation} 

where $K$ is the number of models considered and
$$ \Delta_i(AIC)=AIC_{i}- \textrm{minimum AIC} $$ 

These weights sum to one over all candidate models and can be calculated using your chosen fit score e.g. AIC, AICC or BIC statistics.

## Example: model selection with the medical data

We will now consider model selection in the medical data set. This data set has an interesting diversity of variable types. 

```{r, echo=F}
meddata<- read.csv("data/medical_dataset_for_R_lectures_v1.0.csv")
```

```{r echo=FALSE}
# NOT SHOWN TO STUDENTS
meddata<- meddata[which(is.na(meddata$vitdresul)==FALSE),]
meddata<- meddata[which(meddata$folate<200),]
# removing the irritating whitespaces in the character columns;
meddata$vitdc<-as.factor(gsub(" ", "", meddata$vitdc, fixed = TRUE))
meddata$vitbc<-as.factor(gsub(" ", "", meddata$vitbc, fixed = TRUE))
meddata$teonpres<-ifelse(test = meddata$TEON=='Yes', 1, 0)
```

As a reminder, let's look a the data available in the TEON data set:

```{r}
head(meddata)
```

Let's consider creating a linear model with some covariates of mixed types. Specifically, we'll look at `folate` , `TEON` (presence/absence of TEON), and `gend` (gender) to explain `vitdresul` (vitamin D level). The data is shown in Figure  \@ref(fig:prettierPlot). 

```{r echo=F}
prettierPlot <- ggplot(meddata) + 
  geom_point(aes(folate, vitdresul, colour = gend), size=3, alpha=0.5) + 
  facet_wrap(~TEON)
```

```{r prettierPlot, echo=F, fig.cap="Scatterplot showing the relationships between vitamin D level, folate, TEON and gender."}
prettierPlot
```

### Model specification

Fitting models with `lm` is straight-forward. We specify response, $y$, as a function of several explanatory variables, $x$ - here `vitdresul` is a function of `gend`, `TEON`, (both categorical) and `folate` (numeric). Note, we may have causality the wrong way around here as TEON may be a consequence of vitamin deficiency (but this model serves to illustrate the methods). 

```{r}
multiReg_lm <- lm(vitdresul ~ TEON + gend + folate, data=meddata)
```

We request a summary, as for other `lm`

```{r, eval = T}
summary(multiReg_lm)
```

### Interpreting the parameter estimates

As we have factor covariates, we have to interpret the model coefficients with respect to some baseline level(s). Note `TEON` = no and `Gender` = Female are not listed in the estimates - this is the baseline (Table \@ref(tab:TEONfitted)). 

<caption> (\#tab:TEONfitted) Construction of the fitted equations for all combinations of factor levels. </caption>  
&nbsp;TEON | Gender | Fitted model
------|-------|-----------------------------------------
No | Female | $\widehat{\textrm{vitdresul}} = 18.278 + 0.119\textrm{folate}$
Yes | Female | $\widehat{\textrm{vitdresul}} = 18.278 -8.952 + 0.119\textrm{folate}$
No | Male | $\widehat{\textrm{vitdresul}} = 18.278 -0.104 + 0.119\textrm{folate}$
Yes | Male | $\widehat{\textrm{vitdresul}} = 18.278 -8.952 -0.104 + 0.119\textrm{folate}$

The intercept is 18.27:

+ this is the estimated mean of `vitdresul` when

    - `TEON` = no
    - `gender` = female and 
    - `folate` = 0

+ Further, this is significantly different from zero ($p$-value is $<2\times 10^{-16}$ - effectively zero).

The `TEONYes` parameter is -8.95:

+ this is the difference from the intercept, moving from `TEON` = No to `TEON` = yes
+ it is a decrease of 8.95 units which is statistically significant ($p$-value = $2\times 10^{-7}$).
  
The `gendmale` parameter is -0.104:

+ this is the difference from the intercept, moving from `gender` = Female to `gender` = Male
+ it is a decrease of 0.104 units but is not statistically significant ($p$-value = 0.949).
    
The coefficient for `folate` is 0.119:

+ this is the mean increase in `vitdresul` for a unit increase in `folate`
+ the increase is 0.119 units; not statistically significant ($p$-value = 0.217).    

It is easy to get 95\% CIs for the parameter estimates:

```{r}
confint(multiReg_lm)
```

Consistent with the tests previously (because we're using 0.05 as the _p_-value cutoff):

* zero is not a plausible value for the intercept, or for the `TEON` relationship
* zero _is_ a plausible value for the `folate` and `gender` effects.

Possibly we could remove some variables from the model. 

### What 'should' be in the model? 

We'll now look to select components from/for our model using the methods described previously:

* Selection by _p_-values
* Selection by AIC and similar measures
* Automated selection - forwards, backwards and all-possible-subsets

First, let's try including all variables that are available in the medical data set to explain `vitdresul`. The `.` in the model formula is shorthand for that.

```{r, }
# First fit a model with mixed types and reiterate the interpretation component
library(car)
# Fit everything
bigReg <- lm(vitdresul ~ ., data=meddata)

```

```{r, eval=T}
summary(bigReg)
```

In the summary output above, not all regression coefficients have been estimated (specified by NA) and there is a message about 'singularities'. What has happened? 

* Specifying `y ~ .` should put everything in the data set as a covariate (except the response, obviously). 
* The reason is that we effectively have two (same) covariates indicating the presence/absence of TEON - `TEON` and `teonpres`. 
* This is like having perfectly collinear variables and we cannot estimate both.
* This is one possible source of errors saying _singular_ somewhere in the error.

Let's try again, being a bit more conservative in the variables which are included:

```{r, eval=T}
bigReg <- lm(vitdresul ~ gend + age + vitdc + vit.12 + 
               vitbc + folate + TEON, 
             data=meddata, na.action = na.fail)
summary(bigReg)
```

### What terms are significant?

We can look to get overall tests for the components (rather than examining the individual factor-level estimates).

```{r}
anova(bigReg)
```

* As we have seen previously, with `anova` the order of the variables is important (technically we'd have to work from the bottom up in interpretation). 
* A more practical version is the `Anova` command in the `car` library as mentioned previously. 

```{r}
Anova(bigReg)
```

So what to keep?

* Going backwards, we might drop the least significant - here `vit.12`, by the '-vit.12' indicating remove `vit.12` variable. 
* We can use the `update` function to drop/add terms

```{r, eval=F}
smallerReg <- update(bigReg, .~.-vit.12)
Anova(smallerReg)
```

```{r , echo=F}
smallerReg <- update(bigReg, .~.-vit.12)
Anova(smallerReg)
```

So what to keep?

* Applying the same rationale - drop `vitbc`

```{r eval=F}
smallerReg <- update(smallerReg, .~.-vitbc)
Anova(smallerReg)
```

So on and so forth ...

```{r echo=F}
smallerReg <- update(smallerReg, .~.-vitbc)
Anova(smallerReg)
```

+ Applying the same rationale - we now drop `age`

```{r eval=T}
evensmallerReg <- update(smallerReg, .~.-age)
Anova(evensmallerReg)
```

+ Applying the same rationale again - we now drop `gend`

```{r eval=T}
evenevensmallerReg <- update(evensmallerReg, .~.-gend)
Anova(evenevensmallerReg)
```

+ Applying the same rationale again - we now drop `folate`

```{r eval=T}
evenevenevensmallerReg <- update(evenevensmallerReg, .~.-folate)
Anova(evenevenevensmallerReg)
```

All the terms in this model are associated with a $p$-value <0.05 and so are retained. 

### More automated methods

Rather than fitting models, examining output, changing the model and refitting etc. procedures exist that do all this automatically.  

#### Stepwise selection by AIC

Rather than using the $p$-value for model selection, we saw other criteria such as AIC might be used. We can use the `step` function to apply these automatically:

```{r echo=T}
smallerReg <- step(bigReg)
```

```{r echo=T}
summary(smallerReg)
```

```{r echo=T}
Anova(smallerReg)
```

* We can see at each step all the covariates are considered for exclusion
* The exclusion that gives the lowest AIC is favoured
* The process repeats on this reduced model
* The process terminates when deletions no longer improve the AIC
* We don't necessarily get the same model as other methods e.g. $p$-value deletion. AIC is a different model selection criterion. 

#### All possible subsets

The previous method was an incomplete search - not all models considered. The best model at any point, depends on the previous step, and although efficient is not guaranteed to find our best collection of covariates within our big model specified at the start. 

We can try _all_ possible models (combining up to say 60 terms).

* Remember the `dredge` command for this in the `MuMIn` package:

```{r eval=F}
library(MuMIn)
dredgedReg<- dredge(bigReg)
dredgedReg
```

\tiny
```{r echo=F}
library(MuMIn)
dredgedReg<- dredge(bigReg)
dredgedReg
```

\normalsize

This produces a lot of output. We see:

* The models are ranked on the basis of AICc
* We have coefficients for some covariates, not others - these are the excluded covariates
* We have a $\Delta$ AICc which shows how much the AICc has changed
* We could choose the best model and use it or
* [For a predictive model, we could average a number of these, weighted by AIC - but that is for another course.]

Let's only consider the 'best' models from the set of all possible models, i.e. select the best model and all models with a $\Delta$ AICc < 5 of the best model.

```{r eval=T}
topMods <- get.models(dredgedReg, subset=delta<5)
summary(topMods[[1]])
```
The object `topMods` is a list of models. Items in a list are specified by two square brackets. The best model is in position 1. 
The use of `+1` in the model formula here refers to a normally unstated default in R. `+1` one just means an intercept should be calculated. 

**Q15.2**	Dr X is investigating whether human height can be predicted from measured leg length (again!) and, in addition, index finger length. The data collected are as follows: left leg length (LLL), right leg length (RLL), right finger length (finger), total height (height) and sex at birth (Sex). All length measurements are in cm. Dr X fits the following model. 

```{r eval=F}
Model3 <- lm(height ~ LLL + RLL + finger + Sex, data=df1)
```

Write down the general equation for this model. 

**Q15.3** The summary of the model fitted in Q15.2 is shown below. State the null and alternative hypotheses for testing each regression coefficient.

```{R, eval=F}
Call:
lm(formula = height ~ LLL + RLL + finger + Sex)

Residuals:
    Min      1Q  Median      3Q     Max 
-4.2560 -0.9569 -0.1777  1.2501  4.7053 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)
(Intercept) -2.54854    4.69342  -0.543    0.588
LLL          0.49232    0.35872   1.372    0.173
RLL          0.47476    0.36618   1.297    0.198
finger       0.08138    0.17294   0.471    0.639
Sex         -0.81689    0.61429  -1.330    0.187

Residual standard error: 1.743 on 95 degrees of freedom
Multiple R-squared:  0.877,     Adjusted R-squared:  0.8718 
F-statistic: 169.3 on 4 and 95 DF,  p-value: < 2.2e-16
```

**15.4**	Are any of the explanatory variables significant, testing at a 5% fixed significance level, based on the t-statistics? 
 

**15.5**	The VIF analysis associated with the model is below. 
```{r, eval=F}
vif(model1)
      LLL       RLL    finger       Sex 
94.606522 95.274197  1.077881  1.016933
```
What do you conclude from this VIF analysis and model summary above? 

**Q15.6**	How could you further investigate relationships between explanatory variables? 

**Q15.7** Independently of Dr X, Prof Y is also analysing the data. Prof Y's modelling philosophy is to use AIC for model selection and obtains the following information. 

```{r, eval=F}
model1 <- lm (height ~ LLL + RLL + finger + Sex)
> AIC (model1)
[1] 401.5684

> model2a <- lm (height ~ RLL + finger + Sex)
> AIC (model2a)
[1] 401.7778

> model2b <- lm (height ~ LLL + finger + Sex)
> AIC (model2b)
[1] 401.5684

> model2c <- lm (height ~ LLL + RLL + Sex)
> AIC (model2c)
[1] 400.0473

> model2d <- lm (height ~ LLL + RLL + finger)
> AIC (model2d)
[1] 401.6588
```
Using this information, what would you do next in the modelling process? 

## Summary {#SUMmodsel}

A final note on model selection:

* Be very cautious of automatic model selection tools if you intend to describe/interpret your model because the retention of the variables in an automatic process is really directed towards prediction (rather than explanation) of the observed data.
* Small perturbations in the data (or indeed another sample) can produce a wildly different _model structure_ although the predictions themselves might be quite similar.

Note that implicit in this chapter, is that we want a model for prediction or understanding. In contrast, it may be the whole purpose of the model is to test a particular hypothesis. In this case, model reduction may not be necessary. What needs to be obtained is the probability associated with particular variable of interest as its significance is what is reported. However, we still may want to eliminate extraneous variables to make that particular test as efficient as possible. 

### Learning objectives

At the end of this chapter you should understand: 

1. why model selection can be important, and
2. how model selection is undertaken.

## Answers {#ANSmodsel}

**Q15.1** A low VIF does not indicate a variable should be *in* the model, merely that it is not correlated with another predictor. Its inclusion in the model should still be tested.  

**Q15.2**	\[
height=\ {\beta{}}_0+\ {\beta{}}_1LLL+\ {\beta{}}_2RLL+\ {\beta{}}_3finger+\
{\beta{}}_4Sex+\ \epsilon{}
\]  

**Q15.3**  The hypotheses are:
\raggedright
$H_{0}$: ${\beta{}}_p=0$ (i.e. no relationship between explanatory variable $p$ and the response variable, height) and

\raggedright
$H_{1}$: ${\beta{}}_p\not=0$ (i.e. there is a relationship between variable $p$ and the response and hence, variable is a useful explanatory variable)

**Q15.4** No! All probabilities (Pr(>|t|)) are greater than 0.05. 

**Q15.5**	The VIF scores (and common sense) imply that LLL and RLL are collinear and this correlation leads to a failure to detect a significant leg length effect. Therefore, one of the leg lengths should be removed from the model.   

**Q15.6**	Plotting the explanatory variables against each other would indicate the relationships between them. The pairs function is a useful function for doing this. 

**Q15.7** The better fitting models are the ones with the lowest AIC. Therefore, reject finger as an explanatory variable because the model without finger (i.e. model2c) has the lowest AIC; refit the models using the reduced set of explanatory variables, check the AIC values for the new models.   


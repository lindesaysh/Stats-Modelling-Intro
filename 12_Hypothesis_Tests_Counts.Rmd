<!-- LB update - 04/08/2020 -->

# Tables of counts {#tableofcounts}

## Introduction

Sometimes our data are summarised as tables of counts (or contingency tables); this might be a frequency distribution if there is only one variable (one-way table) or as a cross-tabulation if there are two discrete variables (two-way table). We can still ask questions of these data to test hypotheses and the general approach is similar to that previously described but the reference distribution used is a $\chi^2$ (pronounced 'chi-square') distribution.

Chi-square tests on contingency tables look at the distributions of counts over the cells (in the table) and we ask does a particular row or column distribution differ significantly from some other distribution. We consider two different types of test; a goodness-of-fit test used on a one-way table and a test of independence used on a cross-tabulation. As usual, the validity of conclusions based on these tests rely on some assumptions being met and these are described. 

## $\chi^2$ goodness-of-fit test

As a motivating example, we consider data collected from the Scottish Schools Adolescent Lifestyle and Substance Use Survey (SALSUS) established by the Scottish Executive to monitor substance use among young people in Scotland. School pupils in independent and local authority schools were targeted and the data we use are from 2002 - the total sample size was 22,246 pupils. 

Pupils were asked to record the ethnic group to which they identified, (note that the groups were then combined into five broad categories\footnote{https://www.ethnicity-facts-figures.service.gov.uk/style-guide/ethnic-groups}); the numbers in each group are shown in Table \@ref(tab:pupilgroup).

```{r pupilgroup, echo=F, eval=T}
# Chi-sq goodness of fit example
salsus <- data.frame(Group=c("White", "Asian","Black","Mixed","Other","Total"), Frequency=c(21249, 408, 113, 204, 272, 22246))
pander::pander(salsus, caption="(#tab:pupilgroup) Observed numbers of school pupils in each ethnic group.")
```

We want to assess the similarity of the ethnicity recorded in the SALSUS sample to that of the population in Scotland. Table \@ref(tab:pupilgroup2) shows the proportions of the ethnic groups recorded in the census. 

```{r pupilgroup2, echo=F, eval=T}
census <- data.frame(Group=c("White", "Asian","Black","Mixed","Other"),
                     Proportion=c(0.9726, 0.0180, 0.0014, 0.0050, 0.0030))
pander::pander(census, caption="(#tab:pupilgroup2) Proportion of the population of Scotland in each ethnic group obtained from the census.")
```

As for all tests, we have a null ($H_0$) and alternative hypothesis ($H_1$). In this case we want to test:

$$H_0: \textrm{the ethnicity in the sample is the same as the ethnicity in the population}$$

$$H_1: \textrm{the ethnicity in the sample is not the same as the ethnicity in the population}$$

The first step is to calculate what frequencies we would __expect__ to see if the sample reflected the census population. These expected frequencies for each group (or cell in the table) can be obtained by:

$$\textrm{Expected value} = \textrm{total sample size} \times \textrm{expected cell proportion}$$
Thus, for the 'white' group, the expected value is 

$$ \textrm{Expected value} = 22246 \times 0.9726 = 21636.46$$


These expected values can be obtained for all groups (Table \@ref(tab:pupilgroup3)). 

```{r pupilgroup3, echo=F, eval=T}
# Get rid of total
salsus <- salsus[1:5, ]
salsus$Expected <- sum(salsus$Frequency) * (census$Proportion)
pander::pander(salsus, caption="(#tab:pupilgroup3) The observed frequencies from the SALSUS and the expected values according to the census.")

salsus$t1 <- salsus$Frequency - salsus$Expected
salsus$Chi <- (salsus$t1^2)/salsus$Expected

```

To determine whether the sample data are consistent with the null hypothesis, we calculate a measure of difference between the observed and expected counts using the chi-square test statistic:

$$\chi_{stat}^2 = \sum_{\textrm{all cells}} \frac{(\textrm{observed count} - \textrm{expected count})^2}{\textrm{expected count}}$$
This formula is frequently abbreviated to:

$$\chi_{stat}^2 = \sum_{\textrm{all cells}} \frac{(\textrm{O} - \textrm{E})^2}{\textrm{E}}$$

A key component to this statistic is a simple (squared) distance between what is predicted by our theory (in this case the census), and what we observed in our sample (i.e. O-E in the formula). The chi-square component for the White group is:


$$\chi_{white}^2 = \frac{(21249 - 21636.46)^2}{21636.46}= 6.938$$

The $\chi^2$ contributions are calculated for all ethnic groups (Table \@ref(tab:pupilgroup4)).  

```{r pupilgroup4, echo=F, eval=T}
temp <- salsus[ , c(1:3,5)]
pander::pander(temp, caption="(#tab:pupilgroup4) Chi-square components (column 'Chi') for each ethnic group.")
```

The $\chi^2$ values for each group are added together to give the overall $\chi^2$-test statistic.
$$\chi_{stat}^2 = 6.939 + 0.1432 + 215.14 + 77.37 + 631.3 = 930.9$$

As with other hypothesis tests, the larger the test statistic, the stronger the evidence against $H_0$. Therefore, we wish to know if this test statistic (i.e. $\chi_{stat}^2 = 930.9$) is considered large, if the null hypothesis is true. To determine this, we compare it to a reference distribution; not surprisingly, for a $\chi^2$ test, the reference distribution is a $\chi^2$ distribution. The $\chi^2$ distribution is indexed by one parameter, the degrees of freedom, found from (for a one-way table):

$$\textrm{df} = \textrm{number of categories} - 1 $$

The $\chi^2$ distribution takes different shapes according to the degrees of freedom. You can explore these in Figure \@ref(fig:chishiny). There is also a live version [here](https://moniquemackenzie.shinyapps.io/IntroStats_ChiSq/)


(ref:chishiny) Exploring the $\chi^2$ distribution. You can see a live version by clicking [here](https://moniquemackenzie.shinyapps.io/IntroStats_ChiSq/)

```{r chishiny, echo=FALSE, fig.cap='(ref:chishiny)', screenshot.opts=list(delay=5), dev='png', cache=TRUE}
knitr::include_app("https://moniquemackenzie.shinyapps.io/IntroStats_ChiSq/", height='600px')
```


We have 5 ethnic groups and so 

$$\textrm{df} = 5 - 1 = 4$$ 
The reference distribution ($\chi^2_{df=4}$) is shown in Figure \@ref(fig:chisq1). 

(ref:chisq1) Reference distribution, $\chi^2_{df=4}$. The red shaded area shows the critical region, testing at a 5\% significance level. Note that only one tail is used.

```{r chisq1, echo=F, eval=T, fig.cap="(ref:chisq1)"}
xval <- seq(0, 25, by=0.001)
dval <- dchisq(xval, df=4)
plot(xval, dval, type="l", xlab="Possible values of test statistic", 
     ylab="Density", lwd=2)
abline(h=0)
qval <- qchisq(p=0.05, df=4, lower.tail=F)
newx <- seq(qval, 25, by=0.001)
newd <- dchisq(newx, df=4)
polygon(c(qval, newx, qval), c(0, newd, 0), density=5, col=2, lwd=2)
```

We can see that values most likely to occur are around 2 to 6 and testing at a significance level of 5\%, the critical value is 9.49. Hence, a value of 930.9 is very untypical and, indeed the exact probability associated with this test statistic is pretty much zero. Hence, we have strong evidence against $H_0$; the observed numbers in the ethnic groups from the sample do not represent what we would expect according to the census. 

We can go further and investigate what gave rise to such a large test statistic; $\chi^2$ components for the Black and Other groups were particularly large. The observed number in the Other group was 272 pupils, but according to the census, we expected 67 pupils in this group, hence this large discrepancy led to the large $\chi^2$ component. Thus, pupils identified as Other and Black were over-represented in the sample compared to the population. This may be due to biased sampling or changing demographics in the population. 

### Doing this in R

To perform a $\chi^2$ test in R, we need to provide the observed values and the proportions under the null hypothesis (note, the proportions need to sum to 1). 

```{r, echo=T, eval=T}
# Groups
Group <- c("White","Asian","Black","Mixed","Other")
# Observed frequencies from SALSUS
Frequency <- c(21249, 408, 113, 204, 272)
# Proportions in each group from census
CensusProp <- c(0.9726, 0.018, 0.0014, 0.005, 0.003)

# Chi-square test - save to new object
salsusTest <- chisq.test(x=Frequency, p=CensusProp)
salsusTest
```

The new 'test statistic' object contains some useful information, such as the expected values:

```{r, echo=T, eval=T}
# Expected values
salsusTest$expected
```

Unfortunately, the test statistic object does not contain the $\chi^2$ components but these can easily be calculated:

```{r, echo=T, eval=T}
# Save expected values
Expected <- salsusTest$expected
# Chi-square values for each group
(Frequency - Expected)^2/Expected
```

The critical value associated with testing at a fixed significance level can be found using the following command. Note that the distribution is not symmetric and so we want the area in the right hand tail. 

```{r, echo=T, eval=T}
# Critical value, testing at a 5% significance level
qchisq(p=0.05, df=4, lower.tail=FALSE)
```

The exact $p$-value can be found using:

```{r, echo=T, eval=T}
# Exact p-value for test statistic
pchisq(q=930.9, df=4, lower.tail=FALSE)
```

***

__Q12.1__ A curious child was interested in determining whether a six-sided die was fair and threw the die 60 times and recorded the result each time. The observed frequency distribution is given below. 

```{r, echo=F, eval=T}
dice <- data.frame(Number=1:6,
                      Frequency=c(8, 7, 9, 19, 7, 10))
pander::pander(dice)
```

__a.__ State the null and alternative hypotheses for a chi-square goodness-of-fit test. 

__b.__ For the null hypothesis is part a, what are the expected values? 

__c.__ Calculate a suitable test statistic. 

__d.__ Using the following information, what do you conclude? 

```{r, echo=T, eval=T}
qchisq(p=0.05, df=5, lower.tail=FALSE)
```

__e.__ The child is not convinced by these results of the statistical test. What might they do to convince themselves? 


## $\chi^2$ test of independence

Previously we described a test for a one-way table. Sometimes we have a cross-tabulation, or two-way table. Consider the following data (Table \@ref(tab:voters1)) tabulating support for Democratic, Republican or Independent candidates by gender (taken from [@Agresti2007]). 

```{r voters1, echo=F, eval=T}
vote <- data.frame(Gender=c("Female","Male"),
                   Democrat=c(762,484),Independent=c(327,239),
                   Republican=c(468,477))
pander::pander(vote, caption="(#tab:voters1) Numbers of supporters for each political party by gender.")
```

In total there are 2,757 individuals in the sample. A question that might arise is whether there  is more female support for Democrats than Republicans, for example. Indeed there are more female supporters for Democrats than Republicans in our sample, but there were also more Democrats in the sample. Even adjusting for this, another sample would give different frequencies and so is the difference explicable by sampling variability or is there a relationship between political support and gender? We wish to determine whether there is a relationship, or association, between political support and gender or whether gender and political support independent of one another. 

The null hypothesis we test assumes that the two variables are independent (hence a test of independence). In this example, the null hypothesis is: 

$$H_0: \textrm{gender and voting intention are independent}$$

The alternative hypothesis states the opposite view. 

$$H_1: \textrm{gender and voting intention are not independent, i.e. there is an association between gender and political support.}$$

As before, we calculate expected counts assuming that $H_0$ is true. In this case, the probability of being in a particular cell, is the probability of being in the particular row, multiplied by the probability of being in the particular column (remember $P(A \cap B) = P(A) \times P(B)$ for independent events). Thus, the expected count for each cell in the table is given by

$$\textrm{Expected value} =  \frac{\textrm{row total}}{\textrm{grand total}} \times \frac{\textrm{column total}}{\textrm{grand total}} \times \textrm{grand total}$$
where the grand total is the overall total (or sample size). Some values can be cancelled out and so the formula is abbreviated to:

$$\textrm{Expected value} =  \frac{\textrm{row total} \times \textrm{column total}}{\textrm{grand total}} $$
To use this formula in our example, we need the necessary totals (Table \@ref(tab:voters2)).  

```{r voters2, echo=F, eval=T}
vote <- data.frame(Gender=c("Female","Male","Total"),
                   Democrat=c(762,484, 1246),Independent=c(327,239,566),
                   Republican=c(468,477,945), Total=c(1557,1200,2757))
pander::pander(vote, caption="(#tab:voters2) Numbers of supporters for each political party by gender with the row and column totals added.")
```

Thus, the expected number of female Democrat supporters is:

$$\textrm{Expected value} = \frac{1557 \times 1246}{2757} = 703.67$$

The expected number of male Democrat supporters is:

$$\textrm{Expected value} = \frac{1200 \times 1246}{2757} = 542.33$$

We do this for all cells in the table (Table \@ref(tab:voters3)):

```{r voters3, echo=F, eval=T}
vote.exp <- data.frame(Gender=c("Female","Male"),
                   Democrat=c(703.67, 542.33),Independent=c(319.65, 246.35),
                   Republican=c(533.68, 411.32))
pander::pander(vote.exp, caption="(#tab:voters3) Expected numbers of supporters for each political party by gender.")
```

As before, the test statistic ($\chi_{stat}^2$) is given by 

$$\chi_{stat}^2 = \sum_{\textrm{all cells}} \frac{(\textrm{O} - \textrm{E})^2}{\textrm{E}}$$

The chi-square component for the female Democrat supporters is thus:

$$\frac{(762 - 703.67)^2}{703.67} = 4.84$$

The chi-square values for all cells are shown in Table \@ref(tab:voters4). 

```{r voters4, echo=F, eval=T}
vote.chi <- data.frame(Gender=c("Female","Male"),
                   Democrat=c(4.835, 6.273),Independent=c(0.169,0.220),
                   Republican=c(8.084, 10.489))
pander::pander(vote.chi, caption="(#tab:voters4) Chi-square values for political support by gender.")
```

Thus, the test statistic is:

$$\chi^2_{stat} = 4.835 + 0.169 + 8.084 + 6.273 + 0.220 + 10.489 = 30.07$$

We need to compare this to a $\chi^2$ reference distribution; the degrees of freedom are found from:

$$ \textrm{df} = (\textrm{number of rows}-1) \times (\textrm{number of columns}-1) $$

The data in this example is a $2 \times 3$ table, thus the degrees of freedom are: 
$$ df = (2-1) \times (3-1) = 2$$ 

The reference distribution, $\chi^2_{df=2}$, is shown in Figure \@ref(fig:chisq2). 

(ref:chisq2) Reference distribution, $\chi^2_{df=2}$. The red shaded area shows the critical region, testing at a 5\% significance level.

```{r chisq2, echo=F, eval=T, fig.cap="(ref:chisq2)"}
xval <- seq(0, 12, by=0.001)
dval <- dchisq(xval, df=2)
plot(xval, dval, type="l", xlab="Possible values of test statistic", 
     ylab="Density", lwd=2)
abline(h=0)
abline(v=0)
qval <- qchisq(p=0.05, df=2, lower.tail=F)
newx <- seq(qval, 12, by=0.001)
newd <- dchisq(newx, df=2)
polygon(c(qval, newx, qval), c(0, newd, 0), density=5, col=2, lwd=2)
```

The red shaded area in Figure \@ref(fig:chisq2) indicates that the critical value is 5.99 and so since the test statistic is greater than this, we reject the null hypothesis. There is evidence to suggest that gender is not independent of political support. 

If we examine the $\chi^2$ components (Table \@ref(tab:voters4)), we see that the largest components were for Republicans and then Democrats. There were less female Republicans and more male Republicans than expected given the null hypothesis; the converse was true for Democrats. 

### Doing this in R

The key to performing a $\chi^2$ test of independence is getting the data into the correct matrix form and so it is useful to print it out before going ahead with the test. 

```{r, echo=T, eval=T}
# Create data
voters <- c(762,327,468,484,239,477)
# Convert to a matrix
voters.mat <- matrix(voters, nrow=2, ncol=3, byrow=TRUE)
voters.mat

# Chi-square test of independence
chisq.test(x=voters.mat)
```

Note that for 2 $\times$ 2 tables, a correction is applied by default in the `chisq.test` function. The reason for this is that we assume a discrete distribution can be approximated by continuous distribution (i.e. the $\chi^2$ distribution). To account for this approximation, an adjustment is made to the formula to calculate $\chi_{stat}^2$ which makes the test statistic smaller, and thus the corresponding $p$-value will be larger.  

***

__Q12.2__ A student newspaper (The Saint, 02/02/2019) conducted a survey using social media platforms to determine whether students were in favour of the UK holding a second referendum on Brexit. A statistics student wanted to determine whether voting preference was different on the two social media platforms. The numbers of students who voted are given in the following table.

```{r, echo=F, eval=T}
dat <- data.frame(Platfom=c("Facebook","Twitter"), Yes=c(158,19), No=c(61,11))
pander::pander(dat)
```

__a.__ State a suitable null and alternative hypothesis of a statistical test to determine whether voting preference was different between the social media
platforms.

__b.__ Calculate the expected counts, assuming that voting preference was not related to social media platform.

__c.__ Calculate an appropriate test statistic for the test described in part (a).

__d.__ If the exact $p$-value associated with the test statistic calculated in part (c) was 0.32, what do you conclude?

## Test assumptions

As with other hypothesis tests, $\chi^2$ tests require that some assumptions are fulfilled in order that the results are reliable.$\chi^2$ tests are only valid when the data are collected as a random sample or as a number of random samples. They are 'large' sample tests that require the total count for the table to be sufficiently large. The following rules of thumb help ensure we don't use $\chi^2$ tests for samples which are too small:

+ each expected cell count should be greater than 1
+ 80\% (at least) of the expected counts should be at least 5.

If these rules do not hold, then the categories may be combined in some sensible way to achieve acceptable cell counts. 

## Summary

Although discrete data are summarised and treated differently to continuous data, the approach to hypothesis testing is the same; the null and alternative hypotheses are stated, a test statistic is calculated and then compared to a reference distribution. $\chi^2$ tests are used for discrete data in the form of contingency tables.  

### Learning outcomes

In this chapter you have seen how to undertake a:

1. test for goodness of fit to some a priori distribution, and

2. test for independence/association. 


## Answers

__Q12.1__ In this example, we conduct a goodness-of-fit test to check whether the die is fair. 

__Q12.1__ __a.__ The hypotheses could be specified in a variety of ways, for example

$$H_0: \textrm{all numbers are equally likely} $$
$$H_1: \textrm{the numbers are not equally likely to occur}$$

__Q12.1__ __b.__ The total number of throws was 60. Thus, if each number is equally likely (i.e. with probability $\frac{1}{6}$) we would expect each number to be thrown 10 times (for a sample of size 60). 

__Q12.1__ __c.__ The $\chi^2$ test statistic is given by:

$$\chi_{stat}^2 = \sum_{\textrm{all cells}} \frac{(\textrm{O} - \textrm{E})^2}{\textrm{E}}$$
The $\chi^2$ values are:

```{r, echo=F, eval=T}
dice$Expected <- rep(10, 6)
dice$Chi <- ((dice$Frequency - dice$Expected)^2)/dice$Expected
pander::pander(dice)
```

The $\chi^2$ test statistic is

$$\chi_{stat}^2 = 0.4 + 0.9 + 0.1 + 8.1 + 0.9 + 0 = 10.4$$

__Q12.1__ __d.__ The test statistic is compared to a reference distribution; with six categories, df $= 6 - 1 = 5$. The information provided in the R output was a critical value, testing at a significance level of 5\% (i.e. $\chi^2_{crit}=11.07$). The test statistic is smaller (but not by much) than the critical value, thus these data do not provide evidence to reject the null hypothesis. We conclude that the die is fair.

```{r, echo=T, eval=T}
dieFreq <- c(8, 7, 9, 19, 7, 10)
# Note, no need to specify proportions if expected cell proportions are equal
chisq.test(x=dieFreq)
```

__Q12.1__ __e.__ The child could increase the sample size with more throws of the die. 

__Q12.2__ This question calls for a $\chi^2$ test of independence. 

__Q12.2__ __a.__ The hypotheses are:

$H_0$: Voting preference is independent of social media platform (i.e. no difference in voting preference between platforms)

$H_1$: Voting preference is not independent of the social media platform (i.e. voting preference is different between platforms).

__Q12.2__ __b.__ The expected counts in each cell in the table are given by

$$\textrm{Expected value} = \frac{\textrm{row total} - \textrm{column total}}{\textrm{grand total}}$$
The totals are

```{r, echo=F, eval=T}
dat.tot <- data.frame(Platfom=c("Facebook","Twitter","Total"),
                      Yes=c(158,19,177), No=c(61,11, 72), Total=c(219,30,249))
pander::pander(dat.tot)
```

The expected values are

```{r, echo=F, eval=T}
dat.exp <- data.frame(Platfom=c("Facebook","Twitter"),
                      Yes=c(155.6747,21.3253), No=c(63.3253, 8.6747))
pander::pander(dat.exp)
```

__Q12.2__ __c.__ The chi-square test statistic is given by 

$$\chi_{stat}^2 = \sum_{\textrm{all cells}} \frac{(\textrm{O} - \textrm{E})^2}{\textrm{E}}$$
```{r, echo=F, eval=T}
dat.chi <- data.frame(Platfom=c("Facebook","Twitter"),
                      Yes=c(0.0347,0.2535), No=c(0.0854, 0.6233))
pander::pander(dat.chi)
```

Summing the $\chi^2$ values gives the test statistic:

$$ x_{stat}^2 = 0.0347 + 0.0854 + 0.2535 + 0.6233 = 0.9969$$

__Q12.2__ __d.__ Given that the $p$-value is 0.32 (much greater than a 5\% significance level), there is no evidence to reject the null hypothesis and conclude that the two factors are independent, i.e. that voting preference is independent of choice of social media platform.

```{r, echo=T, eval=T}
voteFreq <- c(158, 61, 19, 11)
voteFreq.mat <- matrix(voteFreq, nrow=2, byrow=TRUE)
# Without correction
chisq.test(x=voteFreq.mat, correct=FALSE)
# With correction
chisq.test(x=voteFreq.mat)
```
